---
title: 机器学习(VII)--强化学习(一)线性回归
date: 
katex: true
categories:
  - Artificial Intelligence
  - Machine Learning
tags:
  - 机器学习
cover: 
top_img: /img/artificial-intelligence.jpg
abbrlink: 
description: 
---



强化学习（Reinforcement Learning，RL）是机器学习中的一个领域，是学习“做什么（即如何把当前的情景映射成动作）才能使得数值化的收益信号最大化”。学习者不会被告知应该采取什么动作，而是必须自己通过尝试去发现哪些动作会产生最丰厚的收益。

强化学习同机器学习领域中的有监督学习和无监督学习不同，有监督学习是从外部监督者提供的带标注训练集中进行学习（任务驱动型），无监督学习是一个典型的寻找未标注数据中隐含结构的过程（数据驱动型）。强化学习是与两者并列的第三种机器学习范式，强化学习带来了一个独有的挑战——“试探”与“开发”之间的折中权衡，智能体必须开发已有的经验来获取收益，同时也要进行试探，使得未来可以获得更好的动作选择空间（即从错误中学习）。

在强化学习中，有两个可以进行交互的对象：智能体（Agnet）和环境（Environment）：

- 智能体：可以感知环境的状态（State），并根据反馈的奖励（Reward）学习选择一个合适的动作（Action），来最大化长期总收益。
- 环境：环境会接收智能体执行的一系列动作，对这一系列动作进行评价并转换为一种可量化的信号反馈给智能体。

除了智能体和环境之外，强化学习系统有四个核心要素：策略（Policy）、回报函数（收益信号，Reward Function）、价值函数（Value Function）和环境模型（Environment Model），其中环境模型是可选的。

- 策略：定义了智能体在特定时间的行为方式。策略是环境状态到动作的映射。
- 回报函数：定义了强化学习问题中的目标。在每一步中，环境向智能体发送一个称为收益的标量数值。
- 价值函数：表示了从长远的角度看什么是好的。一个状态的价值是一个智能体从这个状态开始，对将来累积的总收益的期望。
- 环境模型：是一种对环境的反应模式的模拟，它允许对外部环境的行为进行推断。

强化学习是一种对目标导向的学习与决策问题进行理解和自动化处理的计算方法。它强调智能体通过与环境的直接互动来学习，而不需要可效仿的监督信号或对周围环境的完全建模，因而与其他的计算方法相比具有不同的范式。

强化学习使用马尔可夫决策过程的形式化框架，使用状态，动作和收益定义学习型智能体与环境的互动过程。这个框架力图简单地表示人工智能问题的若干重要特征，这些特征包含了对因果关系的认知，对不确定性的认知，以及对显式目标存在性的认知。

价值与价值函数是强化学习方法的重要特征，价值函数对于策略空间的有效搜索来说十分重要。相比于进化方法以对完整策略的反复评估为引导对策略空间进行直接搜索，使用价值函数是强化学习方法与进化方法的不同之处。
