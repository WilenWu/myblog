# Neural Networks

神经网络

inference (prediction)

## Neurons and the brain

神经元和大脑

神经网络：刚开始是尝试模拟人脑的算法。广泛应用于语音识别、图像识别、NLP等

![神经元简化模型](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221008214017179.png)

![image-20221009210404588](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221009210404588.png)

## Neural Networks

之前，我们用 $f(x)$ 作为算法的输出，为了建立一个神经网络，我们稍微改变下术语，用 $a$ 表示算法的输出a(activation) 为神经科学术语，表示激活的意思。单个神经元(Neuron)所作的是先接收数字 (input) ，带入激活函数，然后输出一个或几个数字 $a$ 。逻辑回归算法可以看作单个神经元简化模型。

![image-20221009212609920](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221009212609920.png)

要建立一个神经网络，只需要把一堆神经元连接起来，或者把他们放在一起。

- 组合在一起的神经元称为一层 (layer)。一层神经元输入相同或相似的特征，然后一起输出数字
- 最后一层神经元的输出就是整个神经网络的预测输出，称为输出层 (output layer)。中间层称为隐藏层(hidden layer)
- 神经元向下游其他神经元的输出称为激活 (activations)，输出值称为激活值 (activation value)
- 多层神经网络也称为多层感知器 (MLP, Multilayer Perceptron)

![神经网络实例](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221009213326708.png)

![image-20221009215203379](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221009215203379.png)

## Neural network model

Neuron with Sigmoid activation
$$
 f_{\mathbf{w},b}(x^{(i)}) = g(\mathbf{w}x^{(i)} + b)
$$
where 
$$
g(x) = sigmoid(x)
$$
符号约定：

![layer 1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221010215559520.png)

![layer 2](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221010215642550.png)

## More complex neural network

![image-20221010221208508](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221010221208508.png)

## Inference: making predictions

- 前向传播算法(forward propagation)：按照神经网络从左到右前进的方向计算。

## Build the model using TensorFlow

![Neural network libraries](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221012191645462.png)

**Tensorflow and Keras**
Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by François Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface.

需要注意 Tensorflow和numpy是以矩阵的格式存储数据

![C2_W1_RoastingNetwork](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/C2_W1_RoastingNetwork.PNG)

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Sequential
from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy
from tensorflow.keras.activations import sigmoid

# 步骤1：构建神经网络
model = Sequential([
        Dense(units=3, activation='relu', name = 'layer1'),
        Dense(units=1, activation='sigmoid', name = 'layer2')
     ])

x = np.array([[200.0, 17.0],
              [120.0, 5.0],
              [425.0, 20.0],
              [212.0, 18.0]])
y = np.array([1,0,0,1])

# 步骤2：用特定损失函数编译模型
model.compile(loss = BinaryCrossentropy())
# 步骤3：训练模型
model.fit(x,y,epochs=100) # epochs 限制梯度下降的步数
# 步骤4：预测
model.predict(x_new)
```

## forward prop in numpy

![C2_W1_dense](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/C2_W1_dense.PNG)

```python
W1_tmp = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] )
b1_tmp = np.array( [-9.82, -9.28,  0.96] )
W2_tmp = np.array( [[-31.18], [-27.59], [-32.56]] )
b2_tmp = np.array( [15.41] )
```



```python
def my_dense(a_in, W, b, g):
    """
    Computes dense layer
    Args:
      a_in (ndarray (n, )) : Data, 1 example 
      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units
      b    (ndarray (j, )) : bias vector, j units  
      g    activation function (e.g. sigmoid, relu..)
    Returns
      a_out (ndarray (j,))  : j units|
    """
    units = W.shape[1]
    a_out = np.zeros(units)
    for j in range(units):               
        w = W[:,j]                                    
        z = np.dot(w, a_in) + b[j]         
        a_out[j] = g(z)               
    return a_out
```

```python
def my_sequential(x, W1, b1, W2, b2):
    a1 = my_dense(x,  W1, b1, sigmoid)
    a2 = my_dense(a1, W2, b2, sigmoid)
    return a2
```

## artificial general intelligence (AGI)

AI

- ANI (artificial narrow intelligence, 弱人工智能) E.g., smart speaker, self-driving car, web search, AI in farming and factories
- AGI (artificial general intelligence, 强人工智能) Do anything a human can do

## loss function

损失函数：BinaryCrossentropy 二元分类交叉熵 （和逻辑损失函数一样）
$$
\text{loss}(f(\mathbf{x}), y)=-y\log f(\mathbf{x})-(1-y)\log(1-f(\mathbf{x}))
$$
Cost Function
$$
J(\mathbf{W},\mathbf{B}) = \frac{1}{m} \sum\limits_{i = 1}^m\text{loss}(f(\mathbf{x}^{(i)}), y^{(i)})
$$

# Activation Functions

## Activation Functions

常见的激活函数

- Linear activation function: $g(z)=z$
- Sigmoid: $g(z)=\cfrac{1}{1+e^{-z}}$
- ReLU (Rectified Linear Unit): $g(z)=\max(0,z)$

![image-20221012191809949](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221012191809949.png)

## Choosing activation functions

Output Layer: 激活函数的选择通常取决于标签 y 的取值

- Binary classification: Sigmoid y=0/1
- Regression: Linear activation function 
- Regression: ReLU y>=0

Hidden Layer: ReLU 激活函数是目前许多从业者训练神经网络中最常见的选择，而很少使用 Sigmoid。原因是 ReLU收敛的更快。

还有其他的激活函数，譬如 tanh 活化函数，LeakyReLU激活函数、swish激活函数

## Why do we need activation functions?

假设对所有隐藏层使用线性激活函数，对输出层使用sigmoid 函数。由于一个线性函数的线性函数本身也是一个线性函，隐藏层可以化简为 $\mathbf{w\cdot x}+b$。所以多层神经网络并不能提升计算复杂特征的能力，也不能学习任何比线性函数更复杂的东西，多层神经网络等价于逻辑回归。所以有个常见的经验法则是，不要在隐藏层中使用线性激活函数，建议使用 ReLU 激活函数。

# Multiclass Classification

多分类问题是指不止两个可能输出标签的分类问题

![C2_W2_mclass_header](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/C2_W2_mclass_header.png)

Logistic regression (2 possible output values) $y=0,1$

$z=\mathbf{w\cdot x}+b$
$a_1=g(z)=\cfrac{1}{1+e^{-z}}=\mathbb P(y=1|\mathbf x)$
$\text{loss}=-y\log a_1-(1-y)\log a_2$
$J(\mathbf w,b)=\text{average loss}$

Softmax regression 是 Logistic regression 的推广 (N possible outputs) $y=1,2,\cdots,N$

$z_j=\mathbf{w_j\cdot x}+b_j\quad j=1,2,\cdots,N$
$a_j=\cfrac{e^{z_j}}{e^{z_1}+e^{z_2}+\cdots+e^{z_N}}=\mathbb P(y=j|\mathbf x)$

note: $a_1+a_2+\cdots+a_N=1$

The softmax function can be written:
$$
a_j = \frac{e^{z_j}}{ \sum_{k=1}^{N}{e^{z_k} }} \tag{1}
$$
The output $\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:
$$
\begin{align}
\mathbf{a}(x) =
\begin{bmatrix}
P(y = 1 | \mathbf{x}; \mathbf{w},b) \\
\vdots \\
P(y = N | \mathbf{x}; \mathbf{w},b)
\end{bmatrix}
=
\frac{1}{ \sum_{k=1}^{N}{e^{z_k} }}
\begin{bmatrix}
e^{z_1} \\
\vdots \\
e^{z_{N}} \\
\end{bmatrix} \tag{2}
\end{align}
$$
The loss function associated with Softmax, the cross-entropy loss, is:
$$
\begin{equation}
  L(\mathbf{a},y)=\begin{cases}
    -\log(a_1), & \text{if } y=1\\
        &\vdots\\
     -\log(a_N), & \text{if } y=N\\
  \end{cases} \tag{3}
\end{equation}
$$
Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. 
$$
\mathbf{1}\{y == n\} =\begin{cases}
    1, & \text{if $y==n$}.\\
    0, & \text{otherwise}.
  \end{cases}
$$
Now the cost is:
$$
\begin{align}
J(\mathbf{w},b) = - \left[ \sum_{i=1}^{m} \sum_{j=1}^{N}  1\left\{y^{(i)} == j\right\} \log \frac{e^{z^{(i)}_j}}{\sum_{k=1}^N e^{z^{(i)}_k} }\right] \tag{4}
\end{align}
$$
Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model = Sequential([
Dense(units=25, activation='relu'),
Dense(units=15, activation='relu'),
Dense(units=10, activation='softmax') ])  # < softmax activation here

model.compile(loss= SparseCategoricalCrossentropy() )
model.fit(X,Y,epochs=100)
```

softmax 的改良代码：减少中间量计算，改良计算精确度

```diff
model = Sequential([
Dense(units=25, activation='relu'),
Dense(units=15, activation='relu'),
- Dense(units=10, activation='softmax') ])
+ Dense(units=10, activation='linear') ])  # <-- Note

- model.compile(loss= SparseCategoricalCrossentropy() )
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
+ loss=SparseCategoricalCrossentropy(from_logits=True) ) # softmax 计算封装在损失函数中
model.fit(X,Y,epochs=100)

+ logits = model(X)
+ f_x = tf.nn.softmax(logits)
```

# Convolutional Neural Networks

![卷积神经网络](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221013215449349.png)

之前接触的都是全连接层(dense layer)，其中这一层是前一层每个激活值的函数。

卷积层(Convolutional Layer)：每个神经元仅能看到前一层的部分激活值。why?

- 能加快计算速度
- 它需要的训练数据更少，也更不容易过拟合

心电图(EKG)实例

![image-20221013220516016](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20221013220516016.png)
