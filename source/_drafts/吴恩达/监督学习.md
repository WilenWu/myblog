# Overfitting

- underfit 欠拟合，也称高偏差 high bias 模型对训练数据拟合不足
- just right 模型拟合的很好。算法也能适用于没出现在样本中的数据，称之为泛化 (generalization)
- overfit 过拟合，也称高方差 high variance

![image-20220925192301827](监督学习.assets/image-20220925192301827.png)

![image-20220925205102329](监督学习.assets/image-20220925205102329.png)



fixes high variance

- 增获得更多的训练样本
- 选择最合适的特征子集（特征选择 feature selection）
- 正则化 Regularization：保留所有的特征，通过缩小特征权重，减少过拟合的风险。

fixes high bias

- 增加额外的特征
- 添加多项式特征 ($x_1^2,x_2^2,x_1x_2,etc$)
- 增大正则化参数 $\lambda$

正则化 Regularization：保留所有的特征，通过缩小特征权重，减少过拟合的风险。例如，带 L^2^ 参数正则化的代价函数
$$
J'(\mathbf w)=J(\mathbf{w})+ \cfrac{\lambda}{2m} \|\mathbf w\|^2
$$
梯度向量
$$
\nabla J'(\mathbf w)=\nabla J(\mathbf{w})+\frac{\lambda}{m} \mathbf w
$$

梯度下降更新方程
$$
\begin{align*}
\mathbf w &=\mathbf w-\alpha\nabla J'(\mathbf w) \\
&=\mathbf w(1-\alpha\frac{\lambda}{m})-\alpha\nabla J(\mathbf{w})
\end{align*}
$$

上式第二项是梯度下降中的普通更新 (usual update) ，正则化项所做的就是在每一次梯度下降迭代时，用$w_j$乘以一个略小于1的数字（假如 $\alpha=0.01,\lambda=1$），来执行普通更新。这也就理解了正则化项为什么可以在每次迭代中缩小 $w_j$ 的值。



# Decision Tree

## Decision Tree Model



## Learning Process

**Decision 1**：如何选择拆分每个节点的特征

- Maximize purity (or minimize impurity)

![image-20221001171705420](%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.assets/image-20221001171705420.png)

## Measuring purity

测量纯度

$P_1$: 正样本比例
$P_0=1-P_1$: 负样本比例

熵(entropy)：对一组数据不纯度的测量
$$
\begin{align*}
H(P_1)&=-P_1\log P_1-P_0\log P_0  \\
&=-P_1\log P_1-(1-P_1)\log (1-P_1)
\end{align*}
$$
{% note info %}$0\log 0 = 0${% endnote %}

![image-20221003184322226](%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.assets/image-20221003184322226.png)

## Choosing a split: Information Gain

信息增益：加权平均熵的减少量

$P_1$: 子分支中正样本的比例

$w$: 根节点到所有子分支的样本比例

![image-20221003184538534](%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.assets/image-20221003184538534.png)
$$
\text{Information Gain}=H(P_1^{\text{left}})-\left(w^{\text{left}}H(P_1^{\text{left}})+w^{\text{right}}H(P_1^{\text{right}})\right)
$$

## Putting it together

Decision Tree Learning

1. 在根节点从所有训练样本开始
2. 计算所有特征的信息增益
3. 选择信息增益最高的特征将数据集划分为左右子分支
4. 继续重复左右分支的分裂过程，直到满足停止条件：
   - 当一个节点100%是一个类别时（零熵）
   - 当分裂一个节点导致树超过最大深度时 (maximum depth)。
   - 如果分裂一个节点导致的信息增益低于阈值。
   - 如果一个节点的样本数低于阈值

> 限制决策树深度和设置阈值的一个原因是通过保持树的小巧不容易导致过拟合

## Using one-hot encoding of categorical features

离散值特征：one-hot encoding

如果一个离散特征可以取 $k$ 个可能的值，我们可以通过创建 $k$ 个取值为0或1的二分类特征来替换。

![image-20221006152557361](%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.assets/image-20221006152557361.png)

## Continuous valued features

连续值特征：一种惯例是对所有样本排序，取排序样本中的所有中点值作为阈值的候选值，取信息增益最高的中点值作为阈值。

## Regression Trees

回归树：叶节点样本的均值作为预测值

衡量划分标准的方法：加权平均方差和的减少

## Tree ensembles

事实证明，只需要改变一个训练样本，可拆分的最高信息增益对应的特征就可能发生改变，因此在根节点会产生一个不同的划分，生成一颗完全不同的树。

因此，单个决策树对数据的微小变化非常敏感。让算法变得更健壮(robust)的一个方法是构建不知一颗决策树，这称之为树集合(Tree ensemble)。

- Using multiple decision trees 多颗树投票决定预测结果
- Sampling with replacement 使用有放回抽样创建新的随机训练集

**Random forest algorithm** 随机森林

Given training set of size $m$

For $b=1$ to $B$

- 使用有放回抽样创建一个大小为 $m$ 的新训练集 ，
- 在新训练集上训练集一颗决策树

让这些树投票决定预测结果。事实证明，让 $B$ 变大不会影响性能，但过了某个点后，你会发现收益递减。这种算法称为袋装决策树(bagged decision tree)。

即使使用放回抽样，有时总是在根节点上使用相同的划分，在根节点附近也有相似的特征组成。所以尝试对算法做进一步的修改，随机每个节点的特征选择，这可能会获得更准确的预测结果。

通常做法是每个节点上选择一个特征进行划分，如果总共有 $n$ 个特征可用，随机选择 $k<n$ 个特征子集，允许算法从这 $k$ 个特征子集中选择最大信息增益的特征来划分。当特征数量大时，通常会选择 $k=\sqrt{n}$ 。这种算法称为随机森林(Random forest)

**XGBoost**

Given training set of size $m$

For $b=1$ to $B$

- 抽样创建一个大小为 $m$ 的新训练集 ，代替从所有的样本等概率抽样（$1/m$），更倾向于选出之前训练的决策树分类错误的样本。（具体概率的数学细节相当复杂）
- 在新训练集上训练集一颗决策树

让这些树投票决定预测结果。种算法称为极端梯度增强 (XGBoost , eXtreme Gradient Boosting)

- 开源
- 增强树的实现，非常快速和高效
- 默认的拆分准则和停止拆分的准则都有很好的选择
- 内置正则化防止过拟合

```python
from xgboost import XGBClassifier 
model = XGBClassifier()
model.fit(X_train, y_train) 
y_pred = model.predict(X_test)
```

## When to use decision trees

